<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Ling Luo</title>
    <meta name="author" content="  " />
    <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
" />


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous" />

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light" />

    <!-- Styles -->
    
    <link rel="shortcut icon" href="/assets/img/favicon.ico"/>
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://rowl1ng.com/">
    
    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark" />

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="https://rowl1ng.com//">Ling Luo</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">About</a>
              </li>
              <!--  -->

              <!-- Other pages -->
              <li class="nav-item dropdown ">
                <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">More</a>
                <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown">
                  <a class="dropdown-item" href="/blog/">Blog</a>
                  <div class="dropdown-divider"></div>
                  <a class="dropdown-item" href="/MyWiki/">Wiki</a>
                </div>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/projects/">Projects</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">Publications</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/teaching/">Teaching</a>
              </li>

              <!-- Toogle theme mode -->
              <div class="toggle-container">
                <a id="light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </a>
              </div>
            </ul>
          </div>
        </div>
      </nav>
    </header>

    <!-- Content -->
    <div class="container mt-5">
      <!-- about.html -->
      <div class="post">
        <header class="post-header">
          <h1 class="post-title">
           Ling Luo
          </h1>
          <p class="desc">PhD Student in SketchX Lab, CVSSP, University of Surrey.</p>
        </header>

        <article>
          <div class="profile float-right">
<figure>

  <picture>
    <source media="(max-width: 480px)" srcset="/assets/img/avatar.gif-480.webp"></source>
    <source media="(max-width: 800px)" srcset="/assets/img/avatar.gif-800.webp"></source>
    <source media="(max-width: 1400px)" srcset="/assets/img/avatar.gif-1400.webp"></source>
    <!-- Fallback to the original file -->
    <img class="img-fluid z-dept-1 rounded" src="/assets/img/avatar.gif" alt="avatar.gif">

  </picture>

</figure>

          </div>

          <div class="clearfix">
            <p>An <a href="https://rowl1ng.com/blog/art/">art lover</a> and a <a href="https://rowl1ng.com/blog/tech/">computer science enthusiast</a>, both contributing to my passion about using computational intelligence to fuel art creation. This passion further motivates me to investigate how humans perceive the three-dimensional world through artistic abstraction.</p>

<p>Iâ€™ve been exploring the possibilities of 3D sketch-assisted content creation using VR technology and machine learning since joining the <a href="http://sketchx.eecs.qmul.ac.uk/" target="_blank" rel="noopener noreferrer">SketchX lab</a> in 2019.
This brand-new research direction is supervised by <a href="http://personal.ee.surrey.ac.uk/Personal/Y.Song/" target="_blank" rel="noopener noreferrer">Prof. Yi-Zhe Song</a> (Director, SketchX Lab, CVSSP, University of Surrey); co-supervised by <a href="https://yulia.gryaditskaya.com" target="_blank" rel="noopener noreferrer">Dr. Yulia Gryaditskaya</a>, <a href="https://scholar.google.co.uk/citations?user=MeS5d4gAAAAJ&amp;hl=en" target="_blank" rel="noopener noreferrer">Prof. Tao Xiang</a> and <a href="https://scholar.google.co.uk/citations?user=F7PtrL8AAAAJ&amp;hl=en" target="_blank" rel="noopener noreferrer">Dr. Yongxin Yang</a>.</p>

<p>I also enjoy <a href="https://rowl1ng.com/MyWiki/">sharing my knowledge</a> and <a href="https://space.bilibili.com/3556743" target="_blank" rel="noopener noreferrer">creating videos</a>.</p>

          </div>

          <!-- News -->          
          <div class="news">
            <h2>News</h2>
            <div class="table-responsive">
              <table class="table table-sm table-borderless"> 
                <tr>
                  <th scope="row">Jan 8, 2024</th>
                  <td>
                    Passed the viva defense!<img class="emoji" title=":mortar_board:" alt=":mortar_board:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f393.png" height="20" width="20">
 
                  </td>
                </tr> 
                <tr>
                  <th scope="row">Mar 22, 2023</th>
                  <td>
                    Sketching into the Metaverse: <a href="https://www.instagram.com/reel/CqGAFGDI3wo/?igshid=YmMyMTA2M2Y%3D" target="_blank" rel="noopener noreferrer">The first fine-grained VR sketch based shape retrieval demo</a> is in AIUK 2023!

 
                  </td>
                </tr> 
                <tr>
                  <th scope="row">Mar 1, 2022</th>
                  <td>
                    <a href="https://cvssp.org/data/VRChairSketch/" target="_blank" rel="noopener noreferrer">The first fine-grained VR Sketch dataset</a> is available now!

 
                  </td>
                </tr> 
                <tr>
                  <th scope="row">Oct 2, 2019</th>
                  <td>
                    Joined SketchX of Surrey! <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20">
 
                  </td>
                </tr> 
              </table>
            </div> 
          </div>

          <!-- Selected papers -->
          <div class="publications">
            <h2>Selected Publications</h2>
            <ol class="bibliography">
<li><div class="row">
  
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bibtex</a>
    


    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>3D shape modeling is labor-intensive and time-consuming and requires years of expertise. Recently, 2D sketches and text inputs were considered as conditional modalities to 3D shape generation networks to facilitate 3D shape modeling. However, text does not contain enough fine-grained information and is more suitable to describe a category or appearance rather than geometry, while 2D sketches are ambiguous, and depicting complex 3D shapes in 2D again requires extensive practice. Instead, we explore virtual reality sketches that are drawn directly in 3D. We assume that the sketches are created by novices, without any art training, and aim to reconstruct physically-plausible 3D shapes. Since such sketches are potentially ambiguous, we tackle the problem of the generation of multiple 3D shapes that follow the input sketch structure. Limited in the size of the training data, we carefully design our method, training the model step-by-step and leveraging multi-modal 3D shape representation. To guarantee the plausibility of generated 3D shapes we leverage the normalizing flow that models the distribution of the latent space of 3D shapes. To encourage the fidelity of the generated 3D models to an input sketch, we propose a dedicated loss that we deploy at different stages of the training process.</p>
    </div>
    

    
    <div class="bibtex hidden">
      <p><figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">luo20233d</span><span class="p">,</span>
  <span class="na">abbr</span> <span class="p">=</span> <span class="s">{ICCV}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{3D VR Sketch Guided 3D Shape Prototyping and Exploration}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Luo, Ling and Chowdhury, Pinaki Nath and Xiang, Tao and Song, Yi-Zhe and Gryaditskaya, Yulia}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the IEEE/CVF International Conference on Computer Vision}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{9267--9276}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{https://openaccess.thecvf.com/content/ICCV2023/papers/Luo_3D_VR_Sketch_Guided_3D_Shape_Prototyping_and_Exploration_ICCV_2023_paper.pdf}</span><span class="p">,</span>
  <span class="na">code</span> <span class="p">=</span> <span class="s">{https://github.com/Rowl1ng/3Dsketch2shape}</span><span class="p">,</span>
  <span class="na">video</span> <span class="p">=</span> <span class="s">{https://www.youtube.com/watch?v=PCig106t7aM&amp;ab_channel=LingLuo}</span><span class="p">,</span>
  <span class="na">slides</span> <span class="p">=</span> <span class="s">{https://drive.google.com/file/d/1HewX7cbp8YTOA0G7b_OsOTolpwIRnS_d/view?usp=sharing}</span><span class="p">,</span>
  <span class="na">poster</span> <span class="p">=</span> <span class="s">{https://drive.google.com/file/d/1xTWyCWziSBq7fwTV5EiCird3_DSmu3xs/view?usp=sharing}</span><span class="p">,</span>
  <span class="na">img</span> <span class="p">=</span> <span class="s">{../assets/img/publication_preview/interp_98.gif}</span><span class="p">,</span>
  <span class="na">arxiv</span> <span class="p">=</span> <span class="s">{2306.10830}</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="s">{true}</span>
<span class="p">}</span></code></pre></figure></p>
    </div>
    
  <div class="col-sm-2 abbr">
  
    <img class="img-fluid z-depth-2 rounded" style="object-fit: contain" src="../assets/img/publication_preview/interp_98.gif">
    
    <abbr class="badge">ICCV</abbr>
    
  
  </div>

  <div id="luo20233d" class="col-sm-8">
    
      <div class="title">3D VR Sketch Guided 3D Shape Prototyping and Exploration</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Luo Ling</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Chowdhury Pinaki Nath,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Xiang Tao,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Song Yi-Zhe,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Gryaditskaya Yulia
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proceedings of the IEEE/CVF International Conference on Computer Vision</em>
      
      
        2023
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/2306.10830" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a>
    
    
    
    
      
      <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Luo_3D_VR_Sketch_Guided_3D_Shape_Prototyping_and_Exploration_ICCV_2023_paper.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
    
    
    <a href="https://www.youtube.com/watch?v=PCig106t7aM&amp;ab_channel=LingLuo" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Video</a>
    
    
      <a href="https://github.com/Rowl1ng/3Dsketch2shape" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
      
      <a href="https://drive.google.com/file/d/1xTWyCWziSBq7fwTV5EiCird3_DSmu3xs/view?usp=sharing" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Poster</a>
      
    
    
      
      <a href="https://drive.google.com/file/d/1HewX7cbp8YTOA0G7b_OsOTolpwIRnS_d/view?usp=sharing" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Slides</a>
      
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>3D shape modeling is labor-intensive and time-consuming and requires years of expertise. Recently, 2D sketches and text inputs were considered as conditional modalities to 3D shape generation networks to facilitate 3D shape modeling. However, text does not contain enough fine-grained information and is more suitable to describe a category or appearance rather than geometry, while 2D sketches are ambiguous, and depicting complex 3D shapes in 2D again requires extensive practice. Instead, we explore virtual reality sketches that are drawn directly in 3D. We assume that the sketches are created by novices, without any art training, and aim to reconstruct physically-plausible 3D shapes. Since such sketches are potentially ambiguous, we tackle the problem of the generation of multiple 3D shapes that follow the input sketch structure. Limited in the size of the training data, we carefully design our method, training the model step-by-step and leveraging multi-modal 3D shape representation. To guarantee the plausibility of generated 3D shapes we leverage the normalizing flow that models the distribution of the latent space of 3D shapes. To encourage the fidelity of the generated 3D models to an input sketch, we propose a dedicated loss that we deploy at different stages of the training process.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div></li>
<li><div class="row">
  
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bibtex</a>
    


    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>We study the practical task of fine-grained 3D-VR-sketch-based 3D shape retrieval. This task is of particular interest as 2D sketches were shown to be effective queries for 2D images. However, due to the domain gap, it remains hard to achieve strong performance in 3D shape retrieval from 2D sketches. Recent work demonstrated the advantage of 3D VR sketching on this task. In our work, we focus on the challenge caused by inherent inaccuracies in 3D VR sketches. We observe that retrieval results obtained with a triplet loss with a fixed margin value, commonly used for retrieval tasks, contain many irrelevant shapes and often just one or few with a similar structure to the query. To mitigate this problem, we for the first time draw a connection between adaptive margin values and shape similarities. In particular, we propose to use a triplet loss with an adaptive margin value driven by a "fitting gap", which is the similarity of two shapes under structure-preserving deformations. We also conduct a user study which confirms that this fitting gap is indeed a suitable criterion to evaluate the structural similarity of shapes. Furthermore, we introduce a dataset of 202 VR sketches for 202 3D shapes drawn from memory rather than from observation.</p>
    </div>
    

    
    <div class="bibtex hidden">
      <p><figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">luo2022</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Structure-Aware 3D VR Sketch to 3D Shape Retrieval}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Luo, Ling and Gryaditskaya, Yulia and Xiang, Tao and Song, Yi-Zhe}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2021 International Conference on 3D Vision (3DV)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">arxiv</span> <span class="p">=</span> <span class="s">{2209.09043}</span><span class="p">,</span>
  <span class="na">supp</span> <span class="p">=</span> <span class="s">{https://drive.google.com/file/d/11rt_fVuqumWUy_jVMAis4di4KW0bRHJr/view?usp=sharing}</span><span class="p">,</span>
  <span class="na">slides</span> <span class="p">=</span> <span class="s">{https://drive.google.com/file/d/1fkKf1N8SoceD_cHdZS0eZutpostHLwyr/view?usp=sharing}</span><span class="p">,</span>
  <span class="na">code</span> <span class="p">=</span> <span class="s">{https://github.com/Rowl1ng/Structure-Aware-VR-Sketch-Shape-Retrieval}</span><span class="p">,</span>
  <span class="na">poster</span> <span class="p">=</span> <span class="s">{https://drive.google.com/file/d/1MdNsnNXScARlap4UdkW8_Nj2tltzVFxH/view?usp=sharing}</span><span class="p">,</span>
  <span class="na">video</span> <span class="p">=</span> <span class="s">{https://youtu.be/hpM9WNLQmjM}</span><span class="p">,</span>
  <span class="na">abbr</span> <span class="p">=</span> <span class="s">{3DV}</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="s">{true}</span>
<span class="p">}</span></code></pre></figure></p>
    </div>
    
  <div class="col-sm-2 abbr">
  
    <img class="img-fluid z-depth-2 rounded" style="object-fit: contain" src="">
    
    <abbr class="badge">3DV</abbr>
    
  
  </div>

  <div id="luo2022" class="col-sm-8">
    
      <div class="title">Structure-Aware 3D VR Sketch to 3D Shape Retrieval</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Luo Ling</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Gryaditskaya Yulia,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Xiang Tao,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Song Yi-Zhe
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In 2021 International Conference on 3D Vision (3DV)</em>
      
      
        2022
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/2209.09043" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a>
    
    
    
    
    
      
      <a href="https://drive.google.com/file/d/11rt_fVuqumWUy_jVMAis4di4KW0bRHJr/view?usp=sharing" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Supp</a>
      
    
    
    
    <a href="https://youtu.be/hpM9WNLQmjM" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Video</a>
    
    
      <a href="https://github.com/Rowl1ng/Structure-Aware-VR-Sketch-Shape-Retrieval" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
      
      <a href="https://drive.google.com/file/d/1MdNsnNXScARlap4UdkW8_Nj2tltzVFxH/view?usp=sharing" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Poster</a>
      
    
    
      
      <a href="https://drive.google.com/file/d/1fkKf1N8SoceD_cHdZS0eZutpostHLwyr/view?usp=sharing" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Slides</a>
      
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>We study the practical task of fine-grained 3D-VR-sketch-based 3D shape retrieval. This task is of particular interest as 2D sketches were shown to be effective queries for 2D images. However, due to the domain gap, it remains hard to achieve strong performance in 3D shape retrieval from 2D sketches. Recent work demonstrated the advantage of 3D VR sketching on this task. In our work, we focus on the challenge caused by inherent inaccuracies in 3D VR sketches. We observe that retrieval results obtained with a triplet loss with a fixed margin value, commonly used for retrieval tasks, contain many irrelevant shapes and often just one or few with a similar structure to the query. To mitigate this problem, we for the first time draw a connection between adaptive margin values and shape similarities. In particular, we propose to use a triplet loss with an adaptive margin value driven by a "fitting gap", which is the similarity of two shapes under structure-preserving deformations. We also conduct a user study which confirms that this fitting gap is indeed a suitable criterion to evaluate the structural similarity of shapes. Furthermore, we introduce a dataset of 202 VR sketches for 202 3D shapes drawn from memory rather than from observation.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div></li>
<li><div class="row">
  
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bibtex</a>
    


    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>We present the first fine-grained dataset of 1,497 3D VR sketch and 3D shape pairs of a chair category with large shapes diversity. Our dataset supports the recent trend in the sketch community on fine-grained data analysis, and extends it to an actively developing 3D domain. We argue for the most convenient sketching scenario where the sketch consists of sparse lines and does not require any sketching skills, prior training or time-consuming accurate drawing. We then, for the first time, study the scenario of fine-grained 3D VR sketch to 3D shape retrieval, as a novel VR sketching application and a proving ground to drive out generic insights to inform future research. By experimenting with carefully selected combinations of design factors on this new problem, we draw important conclusions to help follow-on work. We hope our dataset will enable other novel applications, especially those that require a fine-grained angle such as fine-grained 3D shape reconstruction.</p>
    </div>
    

    
    <div class="bibtex hidden">
      <p><figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">luo2021fine</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Fine-Grained VR Sketching: Dataset and Insights.}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Luo, Ling and Gryaditskaya, Yulia and Yang, Yongxin and Xiang, Tao and Song, Yi-Zhe}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2021 International Conference on 3D Vision (3DV)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1003--1013}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
  <span class="na">arxiv</span> <span class="p">=</span> <span class="s">{2209.10008}</span><span class="p">,</span>
  <span class="na">supp</span> <span class="p">=</span> <span class="s">{https://drive.google.com/file/d/1JXGO1s8pyT7YR26zruDevJwO2XJ4MAE1/view?usp=sharing}</span><span class="p">,</span>
  <span class="na">slides</span> <span class="p">=</span> <span class="s">{https://drive.google.com/file/d/1ENUhdXvaFRC-YdFI9ZMzNZJHLTMEePqM/view?usp=sharing}</span><span class="p">,</span>
  <span class="na">poster</span> <span class="p">=</span> <span class="s">{https://drive.google.com/file/d/1uMKKy-5-uf1p9kY-WM74mORR0fV-ZCah/view?usp=sharing}</span><span class="p">,</span>
  <span class="na">code</span> <span class="p">=</span> <span class="s">{https://github.com/Rowl1ng/Fine-Grained_VR_Sketching}</span><span class="p">,</span>
  <span class="na">website</span> <span class="p">=</span> <span class="s">{https://cvssp.org/data/VRChairSketch}</span><span class="p">,</span>
  <span class="na">img</span> <span class="p">=</span> <span class="s">{../assets/img/publication_preview/3D_sketch.gif}</span><span class="p">,</span>
  <span class="na">abbr</span> <span class="p">=</span> <span class="s">{3DV}</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="s">{true}</span>
<span class="p">}</span></code></pre></figure></p>
    </div>
    
  <div class="col-sm-2 abbr">
  
    <img class="img-fluid z-depth-2 rounded" style="object-fit: contain" src="../assets/img/publication_preview/3D_sketch.gif">
    
    <abbr class="badge">3DV</abbr>
    
  
  </div>

  <div id="luo2021fine" class="col-sm-8">
    
      <div class="title">Fine-Grained VR Sketching: Dataset and Insights.</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Luo Ling</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Gryaditskaya Yulia,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Yang Yongxin,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Xiang Tao,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Song Yi-Zhe
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In 2021 International Conference on 3D Vision (3DV)</em>
      
      
        2021
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/2209.10008" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a>
    
    
    
    
    
      
      <a href="https://drive.google.com/file/d/1JXGO1s8pyT7YR26zruDevJwO2XJ4MAE1/view?usp=sharing" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Supp</a>
      
    
    
    
    
      <a href="https://github.com/Rowl1ng/Fine-Grained_VR_Sketching" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
      
      <a href="https://drive.google.com/file/d/1uMKKy-5-uf1p9kY-WM74mORR0fV-ZCah/view?usp=sharing" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Poster</a>
      
    
    
      
      <a href="https://drive.google.com/file/d/1ENUhdXvaFRC-YdFI9ZMzNZJHLTMEePqM/view?usp=sharing" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Slides</a>
      
    
    
      <a href="https://cvssp.org/data/VRChairSketch" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a>
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>We present the first fine-grained dataset of 1,497 3D VR sketch and 3D shape pairs of a chair category with large shapes diversity. Our dataset supports the recent trend in the sketch community on fine-grained data analysis, and extends it to an actively developing 3D domain. We argue for the most convenient sketching scenario where the sketch consists of sparse lines and does not require any sketching skills, prior training or time-consuming accurate drawing. We then, for the first time, study the scenario of fine-grained 3D VR sketch to 3D shape retrieval, as a novel VR sketching application and a proving ground to drive out generic insights to inform future research. By experimenting with carefully selected combinations of design factors on this new problem, we draw important conclusions to help follow-on work. We hope our dataset will enable other novel applications, especially those that require a fine-grained angle such as fine-grained 3D shape reconstruction.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div></li>
<li><div class="row">
  
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bibtex</a>
    


    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Growing free online 3D shapes collections dictated research on 3D retrieval. Active debate has however been had on (i) what the best input modality is to trigger retrieval, and (ii) the ultimate usage scenario for such retrieval. In this paper, we offer a different perspective towards answering these questions â€“ we study the use of 3D sketches as an input modality and advocate a VR-scenario where retrieval is conducted. Thus, the ultimate vision is that users can freely retrieve a 3D model by air-doodling in a VR environment. As a first stab at this new 3D VR-sketch to 3D shape retrieval problem, we make four contributions. First, we code a VR utility to collect 3D VR-sketches and conduct retrieval. Second, we collect the first set of 167 3D VR-sketches on two shape categories from ModelNet. Third, we propose a novel approach to generate a synthetic dataset of human-like 3D sketches of different abstract levels to train deep networks. At last, we compare the common multi-view and volumetric approaches: We show that, in contrast to 3D shape to 3D shape retrieval, volumetric point-based approaches exhibit superior performance on 3D sketch to 3D shape retrieval due to the sparse and abstract nature of 3D VR-sketches. We believe these contributions will collectively serve as enablers for future attempts at this problem.</p>
    </div>
    

    
    <div class="bibtex hidden">
      <p><figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">luo2020towards</span><span class="p">,</span>
  <span class="na">abbr</span> <span class="p">=</span> <span class="s">{3DV}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Towards 3D VR-sketch to 3D shape retrieval}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Luo, Ling and Gryaditskaya, Yulia and Yang, Yongxin and Xiang, Tao and Song, Yi-Zhe}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2020 International Conference on 3D Vision (3DV)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{81--90}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
  <span class="na">arxiv</span> <span class="p">=</span> <span class="s">{2209.10020}</span><span class="p">,</span>
  <span class="na">supp</span> <span class="p">=</span> <span class="s">{https://drive.google.com/file/d/1nm8Taxc4Ji8iHdcjljygEh3v2yIdmbsO/view?usp=sharing}</span><span class="p">,</span>
  <span class="na">slides</span> <span class="p">=</span> <span class="s">{https://drive.google.com/file/d/1rhba2VxabDFz93II0Xo6e88i7S4d-puN/view?usp=sharing}</span><span class="p">,</span>
  <span class="na">code</span> <span class="p">=</span> <span class="s">{https://github.com/ygryadit/Towards3DVRSketch}</span><span class="p">,</span>
  <span class="na">video</span> <span class="p">=</span> <span class="s">{https://youtu.be/7cdij0rSOaE}</span><span class="p">,</span>
  <span class="na">poster</span> <span class="p">=</span> <span class="s">{https://drive.google.com/file/d/1ZUzV2QIHcsePXaJbzKb-KdNzzGpAWTR6/view?usp=sharing}</span><span class="p">,</span>
  <span class="na">img</span> <span class="p">=</span> <span class="s">{../assets/img/publication_preview/3DV20.gif}</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="s">{true}</span>
<span class="p">}</span></code></pre></figure></p>
    </div>
    
  <div class="col-sm-2 abbr">
  
    <img class="img-fluid z-depth-2 rounded" style="object-fit: contain" src="../assets/img/publication_preview/3DV20.gif">
    
    <abbr class="badge">3DV</abbr>
    
  
  </div>

  <div id="luo2020towards" class="col-sm-8">
    
      <div class="title">Towards 3D VR-sketch to 3D shape retrieval</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Luo Ling</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Gryaditskaya Yulia,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Yang Yongxin,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Xiang Tao,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Song Yi-Zhe
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In 2020 International Conference on 3D Vision (3DV)</em>
      
      
        2020
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/2209.10020" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a>
    
    
    
    
    
      
      <a href="https://drive.google.com/file/d/1nm8Taxc4Ji8iHdcjljygEh3v2yIdmbsO/view?usp=sharing" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Supp</a>
      
    
    
    
    <a href="https://youtu.be/7cdij0rSOaE" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Video</a>
    
    
      <a href="https://github.com/ygryadit/Towards3DVRSketch" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
    
    
      
      <a href="https://drive.google.com/file/d/1ZUzV2QIHcsePXaJbzKb-KdNzzGpAWTR6/view?usp=sharing" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Poster</a>
      
    
    
      
      <a href="https://drive.google.com/file/d/1rhba2VxabDFz93II0Xo6e88i7S4d-puN/view?usp=sharing" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Slides</a>
      
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Growing free online 3D shapes collections dictated research on 3D retrieval. Active debate has however been had on (i) what the best input modality is to trigger retrieval, and (ii) the ultimate usage scenario for such retrieval. In this paper, we offer a different perspective towards answering these questions â€“ we study the use of 3D sketches as an input modality and advocate a VR-scenario where retrieval is conducted. Thus, the ultimate vision is that users can freely retrieve a 3D model by air-doodling in a VR environment. As a first stab at this new 3D VR-sketch to 3D shape retrieval problem, we make four contributions. First, we code a VR utility to collect 3D VR-sketches and conduct retrieval. Second, we collect the first set of 167 3D VR-sketches on two shape categories from ModelNet. Third, we propose a novel approach to generate a synthetic dataset of human-like 3D sketches of different abstract levels to train deep networks. At last, we compare the common multi-view and volumetric approaches: We show that, in contrast to 3D shape to 3D shape retrieval, volumetric point-based approaches exhibit superior performance on 3D sketch to 3D shape retrieval due to the sparse and abstract nature of 3D VR-sketches. We believe these contributions will collectively serve as enablers for future attempts at this problem.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div></li>
<li><div class="row">
  
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bibtex</a>
    


    <!-- Hidden abstract block -->
    

    
    <div class="bibtex hidden">
      <p><figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">zhang2019cascaded</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Cascaded Generative and Discriminative Learning for Microcalcification Detection in Breast Mammograms}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhang, Fandong and Luo, Ling and Sun, Xinwei and Zhou, Zhen and Li, Xiuli and Yu, Yizhou and Wang, Yizhou}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{12578--12586}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{http://openaccess.thecvf.com/content_CVPR_2019/papers/Zhang_Cascaded_Generative_and_Discriminative_Learning_for_Microcalcification_Detection_in_Breast_CVPR_2019_paper.pdf}</span><span class="p">,</span>
  <span class="na">abbr</span> <span class="p">=</span> <span class="s">{CVPR}</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="s">{true}</span>
<span class="p">}</span></code></pre></figure></p>
    </div>
    
  <div class="col-sm-2 abbr">
  
    <img class="img-fluid z-depth-2 rounded" style="object-fit: contain" src="">
    
    <abbr class="badge">CVPR</abbr>
    
  
  </div>

  <div id="zhang2019cascaded" class="col-sm-8">
    
      <div class="title">Cascaded Generative and Discriminative Learning for Microcalcification Detection in Breast Mammograms</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Zhang Fandong,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Luo Ling</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Sun Xinwei,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Zhou Zhen,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Li Xiuli,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Yu Yizhou,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Wang Yizhou
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em>
      
      
        2019
      
      </div>
    

    <div class="links">
    
    
    
    
    
      
      <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Zhang_Cascaded_Generative_and_Discriminative_Learning_for_Microcalcification_Detection_in_Breast_CVPR_2019_paper.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    

    <!-- Hidden bibtex block -->
    
  </div>
</div></li>
</ol>
          </div>

          <!-- Social -->
          <div class="social">
            <div class="contact-icons">
            <a href="mailto:%6C%69%6E%67.%72%6F%77%6C%69%6E%67.%6C%75%6F@%67%6D%61%69%6C.%63%6F%6D" title="email"><i class="fas fa-envelope"></i></a>
            <a href="https://orcid.org/0000-0003-3504-4110" title="ORCID" target="_blank" rel="noopener noreferrer"><i class="ai ai-orcid"></i></a>
            <a href="https://scholar.google.com/citations?user=sqHkGCQAAAAJ" title="Google Scholar" target="_blank" rel="noopener noreferrer"><i class="ai ai-google-scholar"></i></a>
            <a href="https://github.com/Rowl1ng" title="GitHub" target="_blank" rel="noopener noreferrer"><i class="fab fa-github"></i></a>
            <a href="https://www.linkedin.com/in/rowl1ng" title="LinkedIn" target="_blank" rel="noopener noreferrer"><i class="fab fa-linkedin"></i></a>
            <a href="https://twitter.com/LingLuo95" title="Twitter" target="_blank" rel="noopener noreferrer"><i class="fab fa-twitter"></i></a>
            <a href="https://rowl1ng.com//feed.xml" title="RSS Feed"><i class="fas fa-rss-square"></i></a>
            
            </div>

            <div class="contact-note">
              
            </div>
            
          </div>
        </article>

</div>

    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        Â© Copyright 2024   . Powered by <a href="http://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="noopener noreferrer">Unsplash</a>.

      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.2/dist/umd/popper.min.js" integrity="sha256-l/1pMF/+J4TThfgARS6KwWrk/egwuVvhRzfLAMQ6Ds4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js" integrity="sha256-SyTu6CwrfOhaznYZPoolVw2rxoY7lKYKQvqbtqN93HI=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script>
  <script src="/assets/js/zoom.js"></script><!-- Load Common JS -->
  <script src="/assets/js/common.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
  </body>
</html>

